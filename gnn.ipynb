{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "try:\n",
    "    from torch_geometric.data import HeteroData\n",
    "    from torch_geometric.nn import HeteroConv, GATv2Conv\n",
    "except Exception as e:\n",
    "    raise SystemExit(\"Please install torch-geometric and matching torch packages.\")\n",
    "\n",
    "\n",
    "# Utility: sMAPE\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred, eps: float = 1e-8):\n",
    "    # Accept either torch tensors or numpy arrays\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        yt = y_true.abs()\n",
    "        yp = y_pred.abs()\n",
    "        denom = torch.clamp(yt + yp, min=eps)\n",
    "        return torch.mean(2.0 * torch.abs(yp - yt) / denom).item()\n",
    "    else:\n",
    "        import numpy as np\n",
    "        yt = np.abs(y_true)\n",
    "        yp = np.abs(y_pred)\n",
    "        denom = np.clip(yt + yp, eps, None)\n",
    "        return float(np.mean(2.0 * np.abs(yp - yt) / denom))\n",
    "\n",
    "\n",
    "# RCA builders\n",
    "\n",
    "\n",
    "def _rca_binary(flows: pd.DataFrame, role: str) -> pd.DataFrame:\n",
    "    \"\"\"Return binary (RCA>1) matrix: countries x products.\n",
    "    role ∈ {\"export\", \"import\"}\n",
    "    \"\"\"\n",
    "    if role == \"export\":\n",
    "        M = flows.pivot_table(index=\"exporter_id\", columns=\"product_id\", values=\"amount_usd\", aggfunc=\"sum\", fill_value=0.0)\n",
    "    elif role == \"import\":\n",
    "        M = flows.pivot_table(index=\"importer_id\", columns=\"product_id\", values=\"amount_usd\", aggfunc=\"sum\", fill_value=0.0)\n",
    "    else:\n",
    "        raise ValueError(\"role must be 'export' or 'import'\")\n",
    "\n",
    "    country_tot = M.sum(axis=1)\n",
    "    prod_tot = M.sum(axis=0)\n",
    "    grand = float(prod_tot.sum()) if float(prod_tot.sum()) > 0 else np.nan\n",
    "\n",
    "    share_country = M.div(country_tot.replace(0, np.nan), axis=0)\n",
    "    share_global = prod_tot / grand\n",
    "    rca = share_country.div(share_global.replace(0, np.nan), axis=1)\n",
    "    rca_bin = (rca > 1.0).astype(np.float32).fillna(0.0)\n",
    "    return rca_bin\n",
    "\n",
    "\n",
    "# Graph builder (heterogeneous Country/Product graph)\n",
    "\n",
    "\n",
    "def build_hetero_graph_from_rca(flows_2024: pd.DataFrame) -> Tuple[HeteroData, Dict[str, int], Dict[str, int], List[str]]:\n",
    "    \"\"\"Build HeteroData with node types 'country' and 'product'.\n",
    "    Relations: ('country','exports_rca','product') and ('country','imports_rca','product').\n",
    "    Country features: concat [export_RCA_onehot, import_RCA_onehot] over a common product set.\n",
    "    Product nodes start without x (we'll use a learned embedding in the model).\n",
    "    Returns: (data, country2idx, product2idx, product_list)\n",
    "    \"\"\"\n",
    "    # calendar 2024 subset already passed in\n",
    "    rca_exp = _rca_binary(flows_2024, role=\"export\")\n",
    "    rca_imp = _rca_binary(flows_2024, role=\"import\")\n",
    "\n",
    "    # align columns (products)\n",
    "    all_products = sorted(set(rca_exp.columns).union(set(rca_imp.columns)))\n",
    "    rca_exp = rca_exp.reindex(columns=all_products, fill_value=0.0)\n",
    "    rca_imp = rca_imp.reindex(columns=all_products, fill_value=0.0)\n",
    "\n",
    "    # all countries in either matrix\n",
    "    all_countries = sorted(set(rca_exp.index).union(set(rca_imp.index)))\n",
    "    rca_exp = rca_exp.reindex(index=all_countries, fill_value=0.0)\n",
    "    rca_imp = rca_imp.reindex(index=all_countries, fill_value=0.0)\n",
    "\n",
    "    country2idx = {c: i for i, c in enumerate(all_countries)}\n",
    "    product2idx = {p: i for i, p in enumerate(all_products)}\n",
    "\n",
    "    # Country features: concat export-RCA and import-RCA one-hots\n",
    "    X_country = np.concatenate([rca_exp.values, rca_imp.values], axis=1).astype(np.float32)\n",
    "\n",
    "    data = HeteroData()\n",
    "    data[\"country\"].x = torch.tensor(X_country, dtype=torch.float32) # [Nc, 2P]\n",
    "    data[\"product\"].num_nodes = len(all_products)\n",
    "\n",
    "    # Edges: exports_rca and imports_rca (undirected - both directions)\n",
    "    exp_idx = np.argwhere(rca_exp.values >= 1)\n",
    "    src_c = exp_idx[:, 0]\n",
    "    dst_p = exp_idx[:, 1]\n",
    "    data[\"country\", \"exports_rca\", \"product\"].edge_index = torch.tensor(\n",
    "        np.vstack([src_c, dst_p]), dtype=torch.long\n",
    "    )\n",
    "    data[\"product\", \"exports_rca_rev\", \"country\"].edge_index = torch.tensor(\n",
    "        np.vstack([dst_p, src_c]), dtype=torch.long\n",
    "    )\n",
    "\n",
    "    imp_idx = np.argwhere(rca_imp.values >= 1)\n",
    "    src_c2 = imp_idx[:, 0]\n",
    "    dst_p2 = imp_idx[:, 1]\n",
    "    data[\"country\", \"imports_rca\", \"product\"].edge_index = torch.tensor(\n",
    "        np.vstack([src_c2, dst_p2]), dtype=torch.long\n",
    "    )\n",
    "    data[\"product\", \"imports_rca_rev\", \"country\"].edge_index = torch.tensor(\n",
    "        np.vstack([dst_p2, src_c2]), dtype=torch.long\n",
    "    )\n",
    "\n",
    "    return data, country2idx, product2idx, all_products\n",
    "\n",
    "\n",
    "# Dataset building: lags & month features\n",
    "\n",
    "\n",
    "def _prepare_monthly(flows: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = flows.copy()\n",
    "    # make sure'date' is datetime — but don't destroy it if it already is\n",
    "    if not np.issubdtype(df[\"date\"].dtype, np.datetime64):\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"].astype(str), format=\"%Y%m\", errors=\"coerce\")\n",
    "\n",
    "    df[\"ym\"] = df[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "    # Aggregate to monthly country–country–product flows\n",
    "    df = (\n",
    "        df.groupby([\"ym\", \"exporter_id\", \"importer_id\", \"product_id\"], as_index=False)\n",
    "          .agg(amount_usd=(\"amount_usd\", \"sum\"))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_samples_for_holdout(\n",
    "    flows: pd.DataFrame,\n",
    "    holdout_month: str,\n",
    "    W_months: int,\n",
    "    lag_K: int = 3,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Return (train_rows, val_rows) for a holdout month.\n",
    "    - Train rows: feature months t in [T0, Tend], where Tend = hm-2 (targets up to hm-1)\n",
    "    - Val rows:  feature month t = hm-1  (target = hm)\n",
    "    include hm in the working slice only so shift(-1) can form the validation target.\n",
    "    Each row contains: exporter_id, importer_id, product_id, ym (feature month t),\n",
    "    lag1..lagK, m_sin, m_cos, y_pos (for t+1), y_amt (for t+1)\n",
    "    \"\"\"\n",
    "    dfm = _prepare_monthly(flows)\n",
    "    print(\"[HOLDOUT] dfm months:\", dfm[\"ym\"].dt.to_period(\"M\").unique())\n",
    "    hm = pd.Period(holdout_month, freq=\"M\")\n",
    "    Tval = hm - 1               # feature month for validation (t = hm-1)\n",
    "    Tend = (hm - 1) - 1         # last training feature month (hm-2)\n",
    "    T0   = Tend - (W_months - 1)\n",
    "\n",
    "    # Slice enough history for lags and include hm so next_amount for Tval exists\n",
    "    start_for_lags = (T0 - lag_K).to_timestamp()\n",
    "    end_for_lags   = hm.to_timestamp()      # include holdout month for labels only\n",
    "    sli = dfm[(dfm[\"ym\"] >= start_for_lags) & (dfm[\"ym\"] <= end_for_lags)].copy()\n",
    "\n",
    "    # Build lags per triple \n",
    "    sli = sli.sort_values([\"exporter_id\", \"importer_id\", \"product_id\", \"ym\"])  # type: ignore\n",
    "    key = [\"exporter_id\", \"importer_id\", \"product_id\"]\n",
    "    for k in range(1, lag_K + 1):\n",
    "        sli[f\"lag{k}\"] = sli.groupby(key)[\"amount_usd\"].shift(k).fillna(0.0)\n",
    "\n",
    "    # Next-month labels\n",
    "    sli[\"next_amount\"] = sli.groupby(key)[\"amount_usd\"].shift(-1)\n",
    "    sli = sli.dropna(subset=[\"next_amount\"])  # rows with valid t+1\n",
    "\n",
    "    # Month-of-year features (sin/cos)\n",
    "    m = sli[\"ym\"].dt.month\n",
    "    ang = 2 * np.pi * (m - 1) / 12.0\n",
    "    sli[\"m_sin\"], sli[\"m_cos\"] = np.sin(ang), np.cos(ang)\n",
    " \n",
    "    # Targets\n",
    "    sli[\"y_amt\"] = sli[\"next_amount\"].astype(np.float32)\n",
    "    sli[\"y_pos\"] = (sli[\"y_amt\"] > 0).astype(np.float32)\n",
    "\n",
    "    train_rows = sli[(sli[\"ym\"] >= T0.to_timestamp()) & (sli[\"ym\"] <= Tend.to_timestamp())].copy()\n",
    "    val_rows   = sli[sli[\"ym\"] == Tval.to_timestamp()].copy()  # features at hm-1 → target hm\n",
    "\n",
    "    # Safety guards to prevent empty lag/validation issues\n",
    "    for k in range(1, lag_K + 1):\n",
    "        if f\"lag{k}\" not in sli.columns:\n",
    "            sli[f\"lag{k}\"] = 0.0\n",
    "\n",
    "    if train_rows.empty:\n",
    "        print(f\"[WARN] train_rows empty for holdout={holdout_month}; filling dummy 1-row DataFrame.\")\n",
    "        train_rows = sli.head(1).copy()\n",
    "    if val_rows.empty:\n",
    "        print(f\"[WARN] val_rows empty for holdout={holdout_month}; filling dummy 1-row DataFrame.\")\n",
    "        val_rows = sli.head(1).copy()\n",
    "\n",
    "\n",
    "    return train_rows, val_rows\n",
    "\n",
    "\n",
    "# ID mappers & partner selection\n",
    "\n",
    "\n",
    "def build_id_maps(df: pd.DataFrame) -> Tuple[Dict[str, int], Dict[str, int], Dict[str, int]]:\n",
    "    countries = sorted(set(df[\"exporter_id\"]).union(set(df[\"importer_id\"])))\n",
    "    products = sorted(df[\"product_id\"].unique())\n",
    "    c2i = {c: idx for idx, c in enumerate(countries)}\n",
    "    p2i = {p: idx for idx, p in enumerate(products)}\n",
    "    return c2i, c2i, p2i\n",
    "\n",
    "\n",
    "def latest_full_year_slice(flows: pd.DataFrame) -> tuple[pd.DataFrame, int]:\n",
    "    \"\"\"Return a slice for the most recent full calendar year if available (12 months),\n",
    "    otherwise fall back to the latest year present in the data.\"\"\"\n",
    "    s = flows.copy()\n",
    "    s[\"y\"] = s[\"date\"].dt.year\n",
    "    s[\"m\"] = s[\"date\"].dt.month\n",
    "    months_per_year = s.groupby(\"y\")[\"m\"].nunique()\n",
    "    full_years = months_per_year[months_per_year >= 12].index\n",
    "    year = int(full_years.max()) if len(full_years) > 0 else int(s[\"y\"].max())\n",
    "    return flows[flows[\"date\"].dt.year == year], year\n",
    "\n",
    "\n",
    "def top20_partners_by_year(flows_year: pd.DataFrame, anchor: str, direction: str) -> list[str]:\n",
    "    \"\"\"Return top-20 partners for anchor (e.g., 'USA'/'CHN') in a given direction\n",
    "    using the provided year's slice. Only partners with ≥200 HS4 traded with the anchor\n",
    "    (either direction) are considered.\"\"\"\n",
    "    if direction not in {\"import\", \"export\"}:\n",
    "        raise ValueError(\"direction must be 'import' or 'export'\")\n",
    "\n",
    "    f = flows_year.copy()\n",
    "    if direction == \"import\":\n",
    "        pool = f[f[\"importer_id\"].eq(anchor)]\n",
    "        rank = pool.groupby(\"exporter_id\")[\"amount_usd\"].sum()\n",
    "    else:  # export\n",
    "        pool = f[f[\"exporter_id\"].eq(anchor)]\n",
    "        rank = pool.groupby(\"importer_id\")[\"amount_usd\"].sum()\n",
    "\n",
    "    # unique HS4 with positive trade either direction anchor-partner\n",
    "    bread = f[(f[\"exporter_id\"].eq(anchor)) | (f[\"importer_id\"].eq(anchor))]\n",
    "    partner = np.where(bread[\"exporter_id\"].eq(anchor), bread[\"importer_id\"], bread[\"exporter_id\"])\n",
    "    breadth = bread.assign(partner=partner).groupby(\"partner\")[\"product_id\"].nunique()\n",
    "\n",
    "    eligible = set(breadth[breadth >= 200].index.tolist())\n",
    "    ranked = rank[rank.index.isin(eligible)].sort_values(ascending=False)\n",
    "    return ranked.head(20).index.tolist()\n",
    "\n",
    "\n",
    "\n",
    "# PyG Dataset for triples\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TripleBatch:\n",
    "    exp_idx: torch.Tensor\n",
    "    imp_idx: torch.Tensor\n",
    "    prod_idx: torch.Tensor\n",
    "    lags: torch.Tensor\n",
    "    month_feat: torch.Tensor\n",
    "    y_pos: torch.Tensor\n",
    "    y_amt: torch.Tensor\n",
    "\n",
    "class TriplesDataset(Dataset):\n",
    "    def __init__(self, rows: pd.DataFrame, country2nid: Dict[str, int], product2nid: Dict[str, int], lag_K: int):\n",
    "        self.df = rows.reset_index(drop=True)\n",
    "        self.c2n = country2nid\n",
    "        self.p2n = product2nid\n",
    "        self.K = lag_K\n",
    "        # map IDs\n",
    "        self.exp_idx = torch.tensor([self.c2n.get(x, -1) for x in self.df[\"exporter_id\"]], dtype=torch.long)\n",
    "        self.imp_idx = torch.tensor([self.c2n.get(x, -1) for x in self.df[\"importer_id\"]], dtype=torch.long)\n",
    "        self.prod_idx = torch.tensor([self.p2n.get(x, -1) for x in self.df[\"product_id\"]], dtype=torch.long)\n",
    "        # filter out rows with missing mapping\n",
    "        mask = (self.exp_idx >= 0) & (self.imp_idx >= 0) & (self.prod_idx >= 0)\n",
    "        mask_bool = mask.bool()\n",
    "        mask_list = mask_bool.tolist()\n",
    "\n",
    "        self.df = self.df[mask_list].reset_index(drop=True)\n",
    "        self.exp_idx = self.exp_idx[mask_bool]\n",
    "        self.imp_idx = self.imp_idx[mask_bool]\n",
    "        self.prod_idx = self.prod_idx[mask_bool]\n",
    "\n",
    "        # tensors\n",
    "        for k in range(1, self.K + 1):\n",
    "            col = f\"lag{k}\"\n",
    "            if col not in self.df.columns:\n",
    "                print(f\"[WARN] Missing {col}; filling with zeros.\")\n",
    "                self.df[col] = 0.0\n",
    "\n",
    "        lag_cols = [f\"lag{k}\" for k in range(1, self.K + 1)]\n",
    "        self.lags = torch.tensor(self.df[lag_cols].values, dtype=torch.float32)\n",
    "\n",
    "        # month features\n",
    "        self.month_feat = torch.tensor(self.df[[\"m_sin\", \"m_cos\"]].values, dtype=torch.float32)\n",
    "        # labels\n",
    "        self.y_pos = torch.tensor(self.df[\"y_pos\"].values, dtype=torch.float32)\n",
    "        self.y_amt = torch.tensor(self.df[\"y_amt\"].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.exp_idx[idx], self.imp_idx[idx], self.prod_idx[idx],\n",
    "                self.lags[idx], self.month_feat[idx], self.y_pos[idx], self.y_amt[idx])\n",
    "\n",
    "def collate_triples(batch: List[Tuple[torch.Tensor, ...]]) -> TripleBatch:\n",
    "    e, i, p, l, m, yp, ya = zip(*batch)\n",
    "    return TripleBatch(\n",
    "        exp_idx=torch.stack(e),\n",
    "        imp_idx=torch.stack(i),\n",
    "        prod_idx=torch.stack(p),\n",
    "        lags=torch.stack(l),\n",
    "        month_feat=torch.stack(m),\n",
    "        y_pos=torch.stack(yp),\n",
    "        y_amt=torch.stack(ya),\n",
    "    )\n",
    "\n",
    "\n",
    "# Model: Hetero GAT encoder + hurdle heads\n",
    "\n",
    "\n",
    "class TradeHeteroGAT(nn.Module):\n",
    "    def __init__(self, in_dim_country: int, n_products: int, d_model: int = 128, gat_hidden: int = 128,\n",
    "                 gat_heads: int = 4, gat_layers: int = 2, lag_dim: int = 3, time_dim: int = 2, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.proj_country = nn.Linear(in_dim_country, d_model, bias=False)\n",
    "        self.emb_product = nn.Embedding(n_products, d_model)\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norm_country = nn.ModuleList()\n",
    "        self.norm_product = nn.ModuleList()\n",
    "\n",
    "        for _ in range(gat_layers):\n",
    "            conv = HeteroConv({\n",
    "                (\"country\", \"exports_rca\", \"product\"): GATv2Conv((d_model, d_model), gat_hidden // gat_heads, heads=gat_heads, dropout=dropout, add_self_loops=False, concat=True),\n",
    "                (\"product\", \"exports_rca_rev\", \"country\"): GATv2Conv((d_model, d_model), gat_hidden // gat_heads, heads=gat_heads, dropout=dropout, add_self_loops=False, concat=True),\n",
    "                (\"country\", \"imports_rca\", \"product\"): GATv2Conv((d_model, d_model), gat_hidden // gat_heads, heads=gat_heads, dropout=dropout, add_self_loops=False, concat=True),\n",
    "                (\"product\", \"imports_rca_rev\", \"country\"): GATv2Conv((d_model, d_model), gat_hidden // gat_heads, heads=gat_heads, dropout=dropout, add_self_loops=False, concat=True),\n",
    "            }, aggr=\"sum\")\n",
    "            self.convs.append(conv)\n",
    "            self.norm_country.append(nn.LayerNorm(gat_hidden))\n",
    "            self.norm_product.append(nn.LayerNorm(gat_hidden))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.emb_dim = gat_hidden\n",
    "\n",
    "        pair_dim = 2 * self.emb_dim + self.emb_dim + lag_dim + time_dim  #exporter + importer + product + lags + month\n",
    "        hid = 256\n",
    "        self.head_occ = nn.Sequential(\n",
    "            nn.Linear(pair_dim, hid), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hid, hid // 2), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hid // 2, 1)\n",
    "        )\n",
    "        self.head_amt_log1p = nn.Sequential(\n",
    "            nn.Linear(pair_dim, hid), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hid, hid // 2), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hid // 2, 1)\n",
    "        )\n",
    "\n",
    "    def encode(self, data: HeteroData) -> Dict[str, torch.Tensor]:\n",
    "        x_dict = {\n",
    "            \"country\": self.proj_country(data[\"country\"].x.float()),\n",
    "            \"product\": self.emb_product.weight,\n",
    "        }\n",
    "        for conv, ln_c, ln_p in zip(self.convs, self.norm_country, self.norm_product):\n",
    "            x_dict = conv(x_dict, data.edge_index_dict)\n",
    "            # x_dict contains tensors for both types\n",
    "            x_dict[\"country\"] = self.dropout(F.elu(ln_c(x_dict[\"country\"])))\n",
    "            x_dict[\"product\"] = self.dropout(F.elu(ln_p(x_dict[\"product\"])))\n",
    "        return x_dict\n",
    "\n",
    "    def forward(self, data: HeteroData, batch: TripleBatch) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        x = self.encode(data)\n",
    "        he = x[\"country\"][batch.exp_idx]\n",
    "        hi = x[\"country\"][batch.imp_idx]\n",
    "        hp = x[\"product\"][batch.prod_idx]\n",
    "        feat = torch.cat([he, hi, hp, batch.lags, batch.month_feat], dim=-1)\n",
    "        logit = self.head_occ(feat).squeeze(-1)\n",
    "        pred_log1p_amt_pos = self.head_amt_log1p(feat).squeeze(-1)\n",
    "        # unconditional dollars\n",
    "        p = torch.sigmoid(logit)\n",
    "        yhat = p * (torch.exp(pred_log1p_amt_pos).clamp_min(0.0) - 1.0)\n",
    "        return logit, pred_log1p_amt_pos, yhat\n",
    "\n",
    "\n",
    "# Training / evaluation loops\n",
    "\n",
    "\n",
    "def train_epoch(model: TradeHeteroGAT, data: HeteroData, loader: DataLoader, opt: torch.optim.Optimizer,\n",
    "                device: str, lambda_amt: float = 1.0, pos_weight: torch.Tensor | None = None) -> float:\n",
    "    model.train()\n",
    "    bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    huber = nn.SmoothL1Loss()\n",
    "    total = 0.0\n",
    "    for batch in loader:\n",
    "        batch = TripleBatch(\n",
    "            exp_idx=batch.exp_idx.to(device),\n",
    "            imp_idx=batch.imp_idx.to(device),\n",
    "            prod_idx=batch.prod_idx.to(device),\n",
    "            lags=batch.lags.to(device),\n",
    "            month_feat=batch.month_feat.to(device),\n",
    "            y_pos=batch.y_pos.to(device),\n",
    "            y_amt=batch.y_amt.to(device),\n",
    "        )\n",
    "        opt.zero_grad()\n",
    "        logit, pred_log1p, yhat = model(data, batch)\n",
    "        loss_prob = bce(logit, batch.y_pos)\n",
    "        mask = (batch.y_pos > 0.5)\n",
    "        if mask.any():\n",
    "            loss_amt = huber(pred_log1p[mask], torch.log1p(batch.y_amt[mask]))\n",
    "        else:\n",
    "            loss_amt = torch.tensor(0.0, device=device)\n",
    "        loss = loss_prob + lambda_amt * loss_amt\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item()\n",
    "    return total / max(1, len(loader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model: TradeHeteroGAT, data: HeteroData, loader: DataLoader, device: str) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    huber = nn.SmoothL1Loss()\n",
    "    losses, s_list = [], []\n",
    "    for batch in loader:\n",
    "        batch = TripleBatch(\n",
    "            exp_idx=batch.exp_idx.to(device),\n",
    "            imp_idx=batch.imp_idx.to(device),\n",
    "            prod_idx=batch.prod_idx.to(device),\n",
    "            lags=batch.lags.to(device),\n",
    "            month_feat=batch.month_feat.to(device),\n",
    "            y_pos=batch.y_pos.to(device),\n",
    "            y_amt=batch.y_amt.to(device),\n",
    "        )\n",
    "        logit, pred_log1p, yhat = model(data, batch)\n",
    "        loss_prob = bce(logit, batch.y_pos)\n",
    "        mask = (batch.y_pos > 0.5)\n",
    "        if mask.any():\n",
    "            loss_amt = huber(pred_log1p[mask], torch.log1p(batch.y_amt[mask]))\n",
    "        else:\n",
    "            loss_amt = torch.tensor(0.0, device=device)\n",
    "        loss = loss_prob + loss_amt\n",
    "        losses.append(loss.item())\n",
    "        s_list.append(smape(batch.y_amt.detach().cpu(), yhat.detach().cpu()))\n",
    "    return {\"loss\": float(np.mean(losses)), \"sMAPE\": float(np.mean(s_list))}\n",
    "\n",
    "\n",
    "#Backtest windows & final train\n",
    "\n",
    "\n",
    "def fit_predict_tradeflows(\n",
    "    flows: pd.DataFrame,\n",
    "    windows: Iterable[int] = (12, 18, 24),\n",
    "    lag_K: int = 3,\n",
    "    epochs: int = 10,\n",
    "    batch_size: int = 8192,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    print(\"=== FUNCTION CHECK (entering fit_predict_tradeflows) ===\")\n",
    "\n",
    "#Ensure proper datetime dtype before inspecting\n",
    "    flows = flows.copy()\n",
    "    try:\n",
    "        if np.issubdtype(flows[\"date\"].dtype, np.number):\n",
    "            flows[\"date\"] = pd.to_datetime(flows[\"date\"].astype(int).astype(str), format=\"%Y%m\", errors=\"coerce\")\n",
    "        else:\n",
    "            flows[\"date\"] = pd.to_datetime(flows[\"date\"], errors=\"coerce\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not parse 'date' column directly: {e}\")\n",
    "        flows[\"date\"] = pd.to_datetime(flows[\"date\"].astype(str), errors=\"coerce\")\n",
    "\n",
    "    print(\"date dtype after coercion:\", flows[\"date\"].dtype)\n",
    "    print(\"num NaT:\", flows[\"date\"].isna().sum())\n",
    "    if flows[\"date\"].notna().any():\n",
    "        print(\"distinct months:\", flows[\"date\"].dropna().dt.to_period(\"M\").sort_values().unique())\n",
    "        print(\"num distinct months:\", len(flows[\"date\"].dropna().dt.to_period(\"M\").unique()))\n",
    "    else:\n",
    "        raise ValueError(\"All 'date' values failed to parse. Check input format.\")\n",
    "\n",
    "\n",
    "    #Parse dates\n",
    "    flows = flows.copy()\n",
    "    if np.issubdtype(flows[\"date\"].dtype, np.number):\n",
    "        flows[\"date\"] = pd.to_datetime(flows[\"date\"].astype(int).astype(str), format=\"%Y%m\")\n",
    "    else:\n",
    "        flows[\"date\"] = pd.to_datetime(flows[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    min_date, max_date = flows[\"date\"].min(), flows[\"date\"].max()\n",
    "    print(f\"[INFO] Building RCA graph dynamically using data from {min_date:%Y-%m} → {max_date:%Y-%m}\")\n",
    "\n",
    "    #Build RCA graph on all available data\n",
    "    graph, country2nid, product2nid, product_list = build_hetero_graph_from_rca(flows)\n",
    "    graph = graph.to(device)\n",
    "\n",
    "    #Backtest window selection\n",
    "    best = {\"W\": None, \"sMAPE\": 1e9, \"state\": None}\n",
    "    pos_weight_tensor = None\n",
    "\n",
    "    for W in windows:\n",
    "        print(f\"\\n=== Backtest W={W} months ===\")\n",
    "        # holdout automatically uses October 2024 by convention, but can generalize later\n",
    "        train_rows, val_rows = build_samples_for_holdout(flows, holdout_month=\"2024-10\", W_months=W, lag_K=lag_K)\n",
    "\n",
    "        train_ds = TriplesDataset(train_rows, country2nid, product2nid, lag_K)\n",
    "        val_ds = TriplesDataset(val_rows, country2nid, product2nid, lag_K)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_triples)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_triples)\n",
    "\n",
    "        in_dim_country = 2 * len(product_list)\n",
    "        model = TradeHeteroGAT(in_dim_country=in_dim_country, n_products=len(product_list), d_model=128,\n",
    "                               gat_hidden=128, gat_heads=4, gat_layers=2, lag_dim=lag_K, time_dim=2, dropout=0.2).to(device)\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "        pos_rate = float(train_ds.y_pos.mean().item()) if len(train_ds) > 0 else 0.5\n",
    "        pos_weight_tensor = torch.tensor([(1.0 - pos_rate) / max(pos_rate, 1e-6)], device=device)\n",
    "\n",
    "        best_val = {\"sMAPE\": 1e9, \"state\": None}\n",
    "        for ep in range(1, epochs + 1):\n",
    "            tr_loss = train_epoch(model, graph, train_loader, opt, device, lambda_amt=1.0, pos_weight=pos_weight_tensor)\n",
    "            ev = eval_epoch(model, graph, val_loader, device)\n",
    "            print(f\"ep {ep:02d} | train_loss {tr_loss:.4f} | val_loss {ev['loss']:.4f} | val_sMAPE {ev['sMAPE']:.4f}\")\n",
    "            if ev[\"sMAPE\"] < best_val[\"sMAPE\"]:\n",
    "                best_val[\"sMAPE\"] = ev[\"sMAPE\"]\n",
    "                best_val[\"state\"] = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "        if best_val[\"sMAPE\"] < best[\"sMAPE\"]:\n",
    "            best = {\"W\": W, \"sMAPE\": best_val[\"sMAPE\"], \"state\": best_val[\"state\"]}\n",
    "\n",
    "    print(f\"\\n>>> Selected W={best['W']} by best sMAPE={best['sMAPE']:.4f}\")\n",
    "\n",
    "    #Dynamic next-month forecast\n",
    "    all_months = flows[\"date\"].dt.to_period(\"M\").sort_values().unique()\n",
    "    max_month = all_months.max()\n",
    "    if len(all_months) < lag_K + 1:\n",
    "        raise ValueError(f\"Not enough months ({len(all_months)}) for lag_K={lag_K}. Need at least {lag_K+1}.\")\n",
    "    latest_holdout = (max_month + 1).strftime(\"%Y-%m\")\n",
    "    print(f\"[INFO] Data covers {all_months.min()} → {max_month}; forecasting {latest_holdout}\")\n",
    "\n",
    "    #Final training\n",
    "    W_final = int(best[\"W\"])\n",
    "    train_rows, val_rows = build_samples_for_holdout(flows, holdout_month=latest_holdout, W_months=W_final, lag_K=lag_K)\n",
    "\n",
    "    train_ds = TriplesDataset(train_rows, country2nid, product2nid, lag_K)\n",
    "    inf_ds = TriplesDataset(val_rows, country2nid, product2nid, lag_K)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_triples)\n",
    "    inf_loader = DataLoader(inf_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_triples)\n",
    "\n",
    "    in_dim_country = 2 * len(product_list)\n",
    "    final_model = TradeHeteroGAT(in_dim_country=in_dim_country, n_products=len(product_list), d_model=128,\n",
    "                                 gat_hidden=128, gat_heads=4, gat_layers=2, lag_dim=lag_K, time_dim=2, dropout=0.2).to(device)\n",
    "    opt = torch.optim.AdamW(final_model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "    pos_rate = float(train_ds.y_pos.mean().item()) if len(train_ds) > 0 else 0.5\n",
    "    pos_weight_tensor = torch.tensor([(1.0 - pos_rate) / max(pos_rate, 1e-6)], device=device)\n",
    "\n",
    "    best_state, best_loss = None, 1e9\n",
    "    for ep in range(1, epochs + 1):\n",
    "        tr_loss = train_epoch(final_model, graph, train_loader, opt, device, lambda_amt=1.0, pos_weight=pos_weight_tensor)\n",
    "        print(f\"[final] ep {ep:02d} | train_loss {tr_loss:.4f}\")\n",
    "        if tr_loss < best_loss:\n",
    "            best_loss = tr_loss\n",
    "            best_state = {k: v.cpu() for k, v in final_model.state_dict().items()}\n",
    "    if best_state is not None:\n",
    "        final_model.load_state_dict(best_state)\n",
    "\n",
    "    # --- Inference ---\n",
    "    final_model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in inf_loader:\n",
    "            batch = TripleBatch(\n",
    "                exp_idx=batch.exp_idx.to(device),\n",
    "                imp_idx=batch.imp_idx.to(device),\n",
    "                prod_idx=batch.prod_idx.to(device),\n",
    "                lags=batch.lags.to(device),\n",
    "                month_feat=batch.month_feat.to(device),\n",
    "                y_pos=batch.y_pos.to(device),\n",
    "                y_amt=batch.y_amt.to(device),\n",
    "            )\n",
    "            _, _, yhat = final_model(graph, batch)\n",
    "            preds.append(yhat.detach().cpu())\n",
    "\n",
    "    yhat_t = torch.cat(preds).detach().cpu().view(-1)\n",
    "    try:\n",
    "        import numpy as _np  \n",
    "        yhat_arr = yhat_t.numpy()\n",
    "    except Exception:\n",
    "        yhat_arr = yhat_t.tolist()\n",
    "\n",
    "    inf_df = inf_ds.df.copy()\n",
    "    inf_df[\"pred_usd\"] = yhat_arr\n",
    "    inf_df[\"target_month\"] = pd.Period(latest_holdout, freq=\"M\").to_timestamp()\n",
    "\n",
    "    #submission mask using latest full year\n",
    "    flows_year, partner_year = latest_full_year_slice(flows)\n",
    "    us_sources = top20_partners_by_year(flows_year, anchor=\"USA\", direction=\"import\")\n",
    "    us_dests   = top20_partners_by_year(flows_year, anchor=\"USA\", direction=\"export\")\n",
    "    cn_sources = top20_partners_by_year(flows_year, anchor=\"CHN\", direction=\"import\")\n",
    "    cn_dests   = top20_partners_by_year(flows_year, anchor=\"CHN\", direction=\"export\")\n",
    "\n",
    "    m = (\n",
    "        (inf_df[\"importer_id\"].eq(\"USA\") & inf_df[\"exporter_id\"].isin(us_sources)) |\n",
    "        (inf_df[\"exporter_id\"].eq(\"USA\") & inf_df[\"importer_id\"].isin(us_dests))   |\n",
    "        (inf_df[\"importer_id\"].eq(\"CHN\") & inf_df[\"exporter_id\"].isin(cn_sources)) |\n",
    "        (inf_df[\"exporter_id\"].eq(\"CHN\") & inf_df[\"importer_id\"].isin(cn_dests))\n",
    "    )\n",
    "    submission = inf_df[m].copy()\n",
    "    submission = submission[[\"target_month\", \"exporter_id\", \"importer_id\", \"product_id\", \"pred_usd\"]]\n",
    "\n",
    "    return {\n",
    "        \"selected_W\": W_final,\n",
    "        \"graph\": graph,\n",
    "        \"model\": final_model,\n",
    "        \"partner_year\": partner_year,\n",
    "        \"inference_all\": inf_df,\n",
    "        \"submission\": submission,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202301 202503 (18811510, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_trade_file(path, anchor):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # unify province/state column name (not needed later but helps dedup)\n",
    "    if \"province_name\" in df.columns:\n",
    "        df = df.rename(columns={\"province_name\": \"state_name\"})\n",
    "\n",
    "    # standardize column names for the pipeline\n",
    "    df = df.rename(columns={\n",
    "        \"month_id\": \"date\",\n",
    "        \"trade_value\": \"amount_usd\",\n",
    "    })\n",
    "\n",
    "    # direction logic: for Exports, anchor is exporter; for Imports, anchor is importer\n",
    "    df[\"exporter_id\"] = np.where(df[\"trade_flow_name\"] == \"Exports\", anchor, df[\"country_id\"])\n",
    "    df[\"importer_id\"] = np.where(df[\"trade_flow_name\"] == \"Exports\", df[\"country_id\"], anchor)\n",
    "\n",
    "    # keep only what you need\n",
    "    df = df[[\"date\", \"exporter_id\", \"importer_id\", \"product_id\", \"amount_usd\"]]\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- load and combine all four ---\n",
    "usa_2023 = load_trade_file(\"trade_s_usa_state_m_hs_2023.csv\", \"USA\")\n",
    "usa_2024 = load_trade_file(\"trade_s_usa_state_m_hs_2024.csv\", \"USA\")\n",
    "usa_2025 = load_trade_file(\"trade_s_usa_state_m_hs_2025.csv\", \"USA\")\n",
    "chn_2023 = load_trade_file(\"trade_s_chn_m_hs_2023.csv\", \"CHN\")\n",
    "chn_2024 = load_trade_file(\"trade_s_chn_m_hs_2024.csv\", \"CHN\")\n",
    "chn_2025 = load_trade_file(\"trade_s_chn_m_hs_2025.csv\", \"CHN\")\n",
    "\n",
    "flows = pd.concat([usa_2023, usa_2024, usa_2025, chn_2023, chn_2024, chn_2025], ignore_index=True)\n",
    "\n",
    "# remove duplicates if both sources contain the same trade flow entries\n",
    "flows = (\n",
    "    flows.groupby([\"date\", \"exporter_id\", \"importer_id\", \"product_id\"], as_index=False)\n",
    "         .agg(amount_usd=(\"amount_usd\", \"sum\"))\n",
    ")\n",
    "\n",
    "# optional: confirm date coverage\n",
    "print(flows[\"date\"].min(), flows[\"date\"].max(), flows.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FUNCTION CHECK (entering fit_predict_tradeflows) ===\n",
      "date dtype after coercion: datetime64[ns]\n",
      "num NaT: 0\n",
      "distinct months: <PeriodArray>\n",
      "['2023-01', '2023-02', '2023-03', '2023-04', '2023-05', '2023-06', '2023-07',\n",
      " '2023-08', '2023-09', '2023-10', '2023-11', '2023-12', '2024-01', '2024-02',\n",
      " '2024-03', '2024-04', '2024-05', '2024-06', '2024-07', '2024-08', '2024-09',\n",
      " '2024-10', '2024-11', '2024-12', '2025-01', '2025-02', '2025-03']\n",
      "Length: 27, dtype: period[M]\n",
      "num distinct months: 27\n",
      "[INFO] Building RCA graph dynamically using data from 2023-01 → 2025-03\n",
      "\n",
      "=== Backtest W=12 months ===\n",
      "[HOLDOUT] dfm months: <PeriodArray>\n",
      "['2023-01', '2023-02', '2023-03', '2023-04', '2023-05', '2023-06', '2023-07',\n",
      " '2023-08', '2023-09', '2023-10', '2023-11', '2023-12', '2024-01', '2024-02',\n",
      " '2024-03', '2024-04', '2024-05', '2024-06', '2024-07', '2024-08', '2024-09',\n",
      " '2024-10', '2024-11', '2024-12', '2025-01', '2025-02', '2025-03']\n",
      "Length: 27, dtype: period[M]\n",
      "ep 01 | train_loss 704.3120 | val_loss 10.1185 | val_sMAPE nan\n",
      "ep 02 | train_loss 9.3370 | val_loss 9.2208 | val_sMAPE 1.9188\n",
      "ep 03 | train_loss 8.5651 | val_loss 8.5540 | val_sMAPE 1.8971\n",
      "ep 04 | train_loss 7.3840 | val_loss 6.1324 | val_sMAPE nan\n",
      "ep 05 | train_loss 5.7304 | val_loss 5.5499 | val_sMAPE 1.7318\n",
      "ep 06 | train_loss 4.8629 | val_loss 3.5696 | val_sMAPE nan\n",
      "ep 07 | train_loss 4.4268 | val_loss 2.6710 | val_sMAPE nan\n",
      "ep 08 | train_loss 3.7012 | val_loss 2.8731 | val_sMAPE nan\n",
      "ep 09 | train_loss 3.5267 | val_loss 2.1724 | val_sMAPE nan\n",
      "ep 10 | train_loss 3.0556 | val_loss 2.1509 | val_sMAPE nan\n",
      "ep 11 | train_loss 2.8330 | val_loss 1.5497 | val_sMAPE nan\n",
      "ep 12 | train_loss 2.4977 | val_loss 1.3312 | val_sMAPE nan\n",
      "ep 13 | train_loss 2.4492 | val_loss 2.2489 | val_sMAPE nan\n",
      "ep 14 | train_loss 2.2320 | val_loss 1.6816 | val_sMAPE nan\n",
      "ep 15 | train_loss 2.1189 | val_loss 1.8076 | val_sMAPE 1.3557\n",
      "\n",
      "=== Backtest W=18 months ===\n",
      "[HOLDOUT] dfm months: <PeriodArray>\n",
      "['2023-01', '2023-02', '2023-03', '2023-04', '2023-05', '2023-06', '2023-07',\n",
      " '2023-08', '2023-09', '2023-10', '2023-11', '2023-12', '2024-01', '2024-02',\n",
      " '2024-03', '2024-04', '2024-05', '2024-06', '2024-07', '2024-08', '2024-09',\n",
      " '2024-10', '2024-11', '2024-12', '2025-01', '2025-02', '2025-03']\n",
      "Length: 27, dtype: period[M]\n",
      "ep 01 | train_loss 385.4422 | val_loss 9.6172 | val_sMAPE nan\n",
      "ep 02 | train_loss 8.7939 | val_loss 8.8519 | val_sMAPE nan\n",
      "ep 03 | train_loss 7.4115 | val_loss 6.1687 | val_sMAPE 1.8091\n",
      "ep 04 | train_loss 5.4433 | val_loss 3.9683 | val_sMAPE nan\n",
      "ep 05 | train_loss 4.1440 | val_loss 3.4528 | val_sMAPE nan\n",
      "ep 06 | train_loss 3.3337 | val_loss 2.2694 | val_sMAPE nan\n",
      "ep 07 | train_loss 3.0080 | val_loss 1.5625 | val_sMAPE nan\n",
      "ep 08 | train_loss 2.6258 | val_loss 1.5307 | val_sMAPE nan\n",
      "ep 09 | train_loss 2.3042 | val_loss 2.4856 | val_sMAPE 1.6181\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mfit_predict_tradeflows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# backtest 1–2 years of lookback\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlag_K\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# use 3-month lag features\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# moderate training duration\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[87], line 558\u001b[0m, in \u001b[0;36mfit_predict_tradeflows\u001b[0;34m(flows, windows, lag_K, epochs, batch_size, device)\u001b[0m\n\u001b[1;32m    556\u001b[0m best_val \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msMAPE\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1e9\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 558\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_amt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_weight_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m     ev \u001b[38;5;241m=\u001b[39m eval_epoch(model, graph, val_loader, device)\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | train_loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | val_loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mev[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | val_sMAPE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mev[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msMAPE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[87], line 452\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data, loader, opt, device, lambda_amt, pos_weight)\u001b[0m\n\u001b[1;32m    450\u001b[0m     loss_amt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    451\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_prob \u001b[38;5;241m+\u001b[39m lambda_amt \u001b[38;5;241m*\u001b[39m loss_amt\n\u001b[0;32m--> 452\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    454\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/ai_for_trade/trade/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ai_for_trade/trade/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = fit_predict_tradeflows(\n",
    "    flows,\n",
    "    windows=(12, 18, 24),  # backtest 1–2 years of lookback\n",
    "    lag_K=3,                # use 3-month lag features\n",
    "    epochs=15,              \n",
    "    batch_size=8192,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
